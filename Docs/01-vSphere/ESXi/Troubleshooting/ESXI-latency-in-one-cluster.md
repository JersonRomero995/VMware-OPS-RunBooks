# Runbook: ESXi Host Latency Only Inside a Specific Cluster (vSphere 8)

## Scenario:
One ESXi host experiences high latency only when it is inside a specific cluster. When the same host is moved (Disconnected ‚Üí Removed ‚Üí Added) to another cluster in the same vCenter, the latency disappears.

# 1 Validate ESXi Host Health

SSH ‚Üí run:
```
esxcli hardware pci list
esxcli hardware cpuid get
esxcli hardware ipmi sel get
esxtop (press d for disk, n for network)
```
Check:

CPU ready %

Storage latency (DAVG/KAVG/GAVG)

Packet drops in network

If the host only misbehaves inside the cluster, it strongly indicates cluster-level configuration, not host hardware.

# 2 Troubleshooting Deep Dive
# ‚úÖ 2.1 vSAN Enabled Cluster (Most Common Root Cause)

If the problematic cluster has vSAN enabled:

# 2.1.1 Check vSAN Health

vCenter ‚Üí Cluster ‚Üí vSAN ‚Üí Monitor ‚Üí Health & Performance

Check for:

Congestion

Resync throttling

Disk group unhealthy

vSAN network MTU mismatch

Incorrect vSAN NIC teaming

# 2.1.2 Check ESXCLI vSAN Stats

SSH ‚Üí run:
```
esxcli vsan network list
esxcli vsan perf stats get
esxcli vsan health cluster list
```
# 2.1.3 Compare with ‚ÄúWorking‚Äù Cluster

If Cluster B does not run vSAN, the behavior difference makes sense.

Possible Fixes

Fix MTU mismatch (9000 vs 1500)

Verify vSAN vmkernel traffic

Correct vSAN teaming (should avoid LACP in many setups)

Validate multicast/unicast config depending on vSAN version

# ‚úÖ 2.2 DRS or CPU Scheduling Acting Differently

If DRS is configured aggressively or there is contention:

2.2.1 Check CPU Scheduler

SSH ‚Üí run:

esxtop


Check:

%RDY ‚Üí should stay under 5%

%CSTP (co-stop for large vCPU VMs)

%MLMTD indicates CPU limits

# 2.2.2 Cluster-A Settings to Compare

In vCenter:
Cluster ‚Üí Configure ‚Üí vSphere DRS

Check:

Automation level

Scalable shares enabled/disabled

Any CPU or memory reservations applied at resource pool level

Any limits configured

Possible Fixes

Disable/enable DRS

Fix resource pool limits (common silent killer)

Adjust VM reservations

# ‚úÖ 2.3 Resource Pools Misconfiguration (one of the top causes)

Resource pools with incorrect limits can create massive latency for one host while other clusters don‚Äôt have these pools.

Validation

Go to Cluster ‚Üí Resource Allocation ‚Üí Resource Pools ‚Üí Check:

CPU Limit

Memory Limit

Expandable Reservation

A limit = throttling ‚Üí causes latency.

Fix

Remove limits or recreate the resource pool tree.

# ‚úÖ 2.4 EVC Mode Inconsistency

If cluster A has EVC enabled and cluster B does not, the host might run with CPU masking causing scheduling slowdowns.

Check

Cluster A ‚Üí Configure ‚Üí vSphere EVC
Compare with Cluster B.

Fix

Raise/lower EVC to match CPU generation

Disable EVC temporarily to test

Ensure hosts all share same CPU family compatibility

# ‚úÖ 2.5 vSphere HA Heartbeat/Agent Issues

Faulty HA configuration can create cluster-level latency due to repeated agent retries.

Check HA Status

SSH:
```
/etc/init.d/vpxa status
/etc/init.d/hostd status
/etc/init.d/fdm status
```

vCenter:
Cluster ‚Üí Monitor ‚Üí vSphere HA

Fix

Reconfigure cluster for HA

Reinstall HA agent on affected host (Remove from cluster ‚Üí Add again)

# 2.6 Storage Policy or Datastore Connectivity Bound to Cluster

Sometimes storage datastores are attached to a cluster via SPBM policies, multipathing, or specific network paths.

Check Storage Paths

SSH:
```
esxcli storage core device stats get
esxcli storage nmp device list
esxcli storage core path list
```

Compare:

PSP (Round Robin, MRU, Fixed)

ALUA states

Active paths

Fix

Match PSP with other clusters

Fix multipath policy

Validate iSCSI/NFS/vSAN vmkernel binding

# 3 Step-by-Step Troubleshooting Workflow
# Step 1 ‚Äî Compare Cluster Configurations

Export cluster settings for A and B and diff:
Cluster ‚Üí Configure ‚Üí Quickstart ‚Üí Review Hosts

Check for:

HA

DRS

EVC

vSAN

Admission Control

Resource Pools

# Step 2 ‚Äî Put Host in Maintenance Mode in Cluster A

Check if latency persists without running VMs.

If latency persists ‚ûú cluster-level network/storage config.
If latency disappears ‚ûú VM/DRS/resource-pool issue.

# Step 3 ‚Äî Check Networking
```
esxcli network nic list
esxcli network nic stats get
esxcli network vswitch standard list
```

Match MTU, VLAN, NIC teaming with cluster B.

# Step 4 ‚Äî Check Storage Latency on Cluster A
esxtop ‚Üí press d


Check DAVG, KAVG, GAVG.
Compare with cluster B.

# Step 5 ‚Äî Review vCenter Agent Logs

On ESXi:
```
cat /var/log/vmkernel.log
cat /var/log/hostd.log
cat /var/log/vpxa.log
cat /var/log/fdm.log
```

Search for:

APD/PDL

SCSI aborts

NIC drops

vSAN congestion

# üß™ Next Actions (Fast Diagnostic Path)
# üîé 1. Test MTU (most important)
vmkping -I vmk0 -s 8972 x.x.x.x


If fail ‚Üí MTU mismatch = primary cause.

# üîé 2. Check NIC errors
esxcli network nic stats get -n vmnic0


If rcvdrops or txdrops > 0 ‚Üí physical switch issue.

# üîé 3. Compare vSwitch or vDS settings between clusters

Same MTU?

Same VLAN?

Same LACP settings?

Same load-balancing policy?

# üîé 4. Trace a route to verify path difference
vmkping --netstack=default x.x.x.x

#üöÄ What I Would Do If I Were Troubleshooting Live (Step-by-Step)
#Step 1: Compare MTU

Bad host:

esxcfg-vmknic -l


Good host:

esxcfg-vmknic -l


‚Üí Confirm if Cluster A = 9000 && Cluster B = 1500.

# Step 2: Check NIC link state
ethtool vmnic0

# Step 3: Check VLAN trunking on DVS

Go to vCenter ‚Üí Networking

Compare Cluster A dvSwitch vs Cluster B dvSwitch.

# Step 4: Ask network team

Which switch ‚Üí which ports ‚Üí which VLAN trunk ‚Üí which MTU?

# Another possible situation from the network end and MTU mismatch if the vmkping result is similar to this:

Look at your test:

vmkping -I vmk0 -s 8972 <target host with issues>


Results:

66% packet loss

~170 ms latency

Only 1 packet succeeded

This only happens inside that one cluster

Good cluster = zero issues

This might confirm:

# ‚ùå The physical network links/uplinks/VLAN trunk for that cluster are NOT supporting jumbo frames (MTU 9000), even though the ESXi side expects MTU 9000.
#‚úî The other cluster (working cluster) does have a clean jumbo frame path.
üéØ What This Means

Your cluster has at least one of these problems:

# 1. Physical switch ports for Cluster A are set to MTU 1500

While ESXi vmkernel interfaces (vmk0/vmk1/etc.) are configured for MTU 9000.

# 2. The VLAN used by that vmkernel path is not configured for jumbo frames

Even if other VLANs are.

# 3. LACP or port-channel on Cluster A switches is incorrectly configured

‚Äî mismatched physical ports
‚Äî one port with MTU 1500, the others 9000
This causes packet fragmentation + loss + latency spikes.

# 4. Cluster A uses a different DVS or different Uplink Profile (Netstack)

One profile has MTU 1500, the other MTU 9000.

# 5. A misconfigured router or firewall between hosts

If the gateway or intermediate device does not support MTU 9000.

# üß™ How To Confirm Which Device Is Dropping the Packets
# Step 1 ‚Äî Find the vmkernel you used
esxcfg-vmknic -l


Identify:

vmkX

VLAN ID

MTU

Compare with the working cluster.

# Step 2 ‚Äî Check the switch uplinks used by that vmkernel

On the bad host:

esxcfg-vswitch -l


If using vDS:

esxcli network vswitch dvs vmware list


Look for:

Uplink assignment

MTU on vDS

VLAN trunking list

# Step 3 ‚Äî Verify MTU on vmnic
esxcli network nic list


If speed/duplex is weird:

100/full

1000/half

flapping

‚Üí Another strong indicator.

# Step 4 ‚Äî Check NIC packet drops
esxcli network nic stats get -n vmnicX


If you see high txDrops or rxDrops ‚Üí physical network mismatch.

üõ†Ô∏è Fix (Network Team Required)

# If all path down error messages are found
This means the host is losing ALL its paths to a datastore, even temporarily, and that can absolutely cause:

high latency

VM freezes

host lockups

vSAN resync storms

vMotion failures

ICMP jitter (because kernel is stuck waiting for I/O)

So now we treat this as a storage path failure, not only network.

Here is exactly what you can perform.

# ‚úÖ 1. Confirm APD in Logs

The APD message is always in vmkernel.log.

Search for:
```
grep -i "APD" /var/log/vmkernel.log
grep -i "All Paths Down" /var/log/vmkernel.log
grep -i "NMP" /var/log/vmkernel.log
```

Common APD patterns:
```
NMP: nmp_DeviceAttemptFailover: Retry world
APD Timeout
No Paths to device
WARNING: Path status changed to Dead
Device <naa.xxx> in APD state
```

This tells you:

which datastore/device

when the APD started

how long it lasted

which paths died

# ‚úÖ 2. Identify the Affected Device

Run:

esxcli storage core device list


Find the device (naa.xxx) reported in the APD log.

Then check path state:

esxcli storage core path list -d <naa.xxx>


APD symptoms:

All paths = Dead

No active I/O

Path state changing repeatedly

Recoverable after failover

# ‚úÖ 3. Determine the Type: APD vs PDL

Both are fatal but different:

APD (All Paths Down) = ESXi thinks storage is gone but doesn't know why.

Temporary ‚Üí recoverable.

PDL (Permanent Device Loss) = Storage array TOLD ESXi ‚ÄúI‚Äôm gone.‚Äù

Not recoverable.

Check:

grep -i "PDL" /var/log/vmkernel.log

# ‚úÖ 4. Actions You Can Perform Immediately
# ‚úî 4.1 Restart I/O stack on ESXi

This often clears stale paths:
```
/etc/init.d/devfs restart
/etc/init.d/vmkernel-storage-migrated restart
/etc/init.d/storageRM restart
```

Do NOT reboot at this stage unless required.

# ‚úî 4.2 Rescan Storage
```
esxcli storage core adapter rescan --all
esxcli storage core device rescan --all
```

This forces ESXi to rediscover paths.

# ‚úî 4.3 Check Network Backing for iSCSI, NFS, or vSAN

Since you‚Äôve shown packet loss and MTU issues, this is likely the root cause of APD.

Check vmkernel interfaces:

esxcfg-vmknic -l


Check NIC errors:

esxcli network nic stats get -n vmnicX


If ANY rx/tx errors increase ‚Üí storage traffic is being dropped ‚Üí leads to APD.

# ‚úî 4.4 Check iSCSI Sessions (if using iSCSI)
esxcli iscsi session list


Symptoms:

Sessions flapping

Missing portals

Connection timeout

# ‚úî 4.5 Check NFS Mount State (if using NFS)
esxcli storage nfs list


Symptoms:

Inactive state

Timeouts

# ‚úî 4.6 For vSAN Clusters
esxcli vsan health cluster list


Look for:

vSAN network down

Disk group offline

Resync throttling due to MTU issues


